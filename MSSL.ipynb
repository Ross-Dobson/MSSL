{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_SHe49EJ7Dt",
    "outputId": "75110f99-3d48-454f-a48a-257524182ce6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  # remember to run 'matplotlib tk'\n",
    "import matplotlib.dates as dt\n",
    "import pathlib  # used for compatibility with non-POSIX systems\n",
    "import datetime\n",
    "import time\n",
    "import urllib.request\n",
    "import os\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (6, 4.5)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, KFold\n",
    "from sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "use_tpu = True\n",
    "if  use_tpu:\n",
    "    assert 'COLAB_TPU_ADDR' in os.environ, 'No TPU; did you request one?'\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "    TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
    "else:\n",
    "    TF_MASTER=''\n",
    "tpu_address = TF_MASTER\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "\n",
    "def import_omni_month(year, month, resolution='1min', cols='All'):\n",
    "    \"\"\"\n",
    "    Finds local OMNI Data files, if not available attempts to download.\n",
    "    Downloads files from\n",
    "    https://spdf.gsfc.nasa.gov/pub/data/omni/high_res_omni/monthly_1min/\n",
    "    Rules of the road at https://omniweb.sci.gsfc.nasa.gov/html/citing.html\n",
    "    Args:\n",
    "        year: the year of the data to import\n",
    "        month: the month of the data to import\n",
    "        resolution: 1min or 5min (only 1min implemented)\n",
    "    Returns:\n",
    "        data: a pandas DataFrame object.\n",
    "    Author: Andy Smith\n",
    "    Updated: Ross Dobson, August 2020\n",
    "    \"\"\"\n",
    "\n",
    "    # get string version of the year and month to use in the filenames\n",
    "    year_str = str(year)\n",
    "    if month < 10:\n",
    "        month_str = '0'+str(month)  # needs to be e.g. \"05\" not \"5\"\n",
    "    else:\n",
    "        month_str = str(month)\n",
    "\n",
    "    leap_year = leapcheck(year)  # is it a leap year (is feb 28 or 29 days)\n",
    "\n",
    "    # List end day for each month so we can cycle through\n",
    "    month_end_dates = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    if leap_year:\n",
    "        month_end_dates[1] = 29  # feb now has 29 as leap year\n",
    "\n",
    "    # make datetime objects. End needs to be 23:59:59 to get all of last day\n",
    "    start_datetime = datetime.datetime(year, month, 1)\n",
    "    end_datetime = datetime.datetime(year, month, month_end_dates[(month-1)],\n",
    "                                     23, 59, 59)\n",
    "\n",
    "    # See https://spdf.gsfc.nasa.gov/pub/data/omni/high_res_omni/hroformat.txt\n",
    "    omni_header = ['Year', 'Day', 'Hour', 'Minute', 'B_IMF_ScID',\n",
    "                   'Plasma_ScID', 'IMFAvPoints', 'PlasmaAvPoints',\n",
    "                   'PercentInterp', 'Timeshift', 'RMSTimeshift',\n",
    "                   'RMSPhaseFrontNormal', 'TimeBetweenObs', 'B', 'B_X_GSM',\n",
    "                   'B_Y_GSE', 'B_Z_GSE', 'B_Y_GSM', 'B_Z_GSM', 'RMSBScalar',\n",
    "                   'RMSFieldVector', 'V', 'V_X_GSE', 'V_Y_GSE', 'V_Z_GSE',\n",
    "                   'n_p', 'T', 'P', 'E', 'Beta', 'AlfvenMachff', 'X_SC_GSE',\n",
    "                   'Y_SC_GSE', 'Z_SC_GSE', 'X_BSN_GSE', 'Y_BSN_GSE',\n",
    "                   'Z_BSN_GSE', 'AE', 'AL', 'AU', 'SymD', 'SymH', 'AsyD',\n",
    "                   'AsyH', 'PCN', 'MagnetosonicMach', '10MeVProton',\n",
    "                   '30MeVProton', '60MeVProton']\n",
    "\n",
    "    # Use pickles, MUCH faster than the .asc files\n",
    "    pkl_dir = pathlib.Path('/content/drive/My Drive/MSSL_DATA/pkl')\n",
    "\n",
    "    mon_str = str(month)\n",
    "    if (month < 10):\n",
    "        mon_str = '0' + str(month)\n",
    "\n",
    "    pkl_fname = str(year) + mon_str + '.pkl'\n",
    "    pkl_path = pkl_dir / pkl_fname\n",
    "\n",
    "    # Look for the pickle\n",
    "    try:\n",
    "        data = pd.read_pickle(pkl_path)\n",
    "        print(\"Pickle found:\", pkl_fname)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pickle not found. Looking for local .asc files to create it.\")\n",
    "\n",
    "        # Check if already downloaded because these files are big\n",
    "        asc_dir = pathlib.Path('/content/drive/My Drive/MSSL_DATA/asc')\n",
    "        asc_fname = 'OMNI_1min_' + year_str + month_str + '.asc'\n",
    "        asc_path = asc_dir / asc_fname\n",
    "        try:\n",
    "            # headers are NOT stored in the NASA asc data files, so header=None\n",
    "            # instead, manually passed in via 'names'. Not ideal, but it works\n",
    "            data = pd.read_csv(asc_path, sep='\\s+',\n",
    "                               names=omni_header, header=None)\n",
    "            print('Local data found at', asc_path)\n",
    "\n",
    "            # not entirely sure what this is doing - just generating the\n",
    "            # column of datetimes to use as the index?\n",
    "            data['DateTime'] = data.apply(\n",
    "                lambda row:\n",
    "                datetime.datetime(int(row.Year), 1, 1)\n",
    "                + datetime.timedelta(\n",
    "                    days=int(row.Day) - 1)\n",
    "                + datetime.timedelta(seconds=row.Hour*60*60 + row.Minute*60),\n",
    "                axis=1)\n",
    "\n",
    "        # FileNotFoundError from pd.read_csv means we need to download the data\n",
    "        except FileNotFoundError:\n",
    "\n",
    "            print('Local .asc not found -> '\n",
    "                  + 'downloading from https://spdf.sci.gsfc.nasa.gov')\n",
    "            asc_url = ('https://spdf.sci.gsfc.nasa.gov/pub/data/omni/'\n",
    "                       + 'high_res_omni/monthly_1min/omni_min'\n",
    "                       + year_str + month_str + '.asc')\n",
    "\n",
    "            print('Creating local directory at ', asc_dir,\n",
    "                  ' (if it doesn\\'t already exist)')\n",
    "            pathlib.Path(asc_dir).mkdir(exist_ok=True)\n",
    "\n",
    "            print('Done. Downloading data to: ', asc_path)\n",
    "            urllib.request.urlretrieve(asc_url, asc_path)  # Saves to asc_path\n",
    "            print('Data downloaded.')\n",
    "\n",
    "        # headers NOT in data, passed in via 'names' parameter instead\n",
    "        data = pd.read_csv(asc_path, sep='\\s+',\n",
    "                           names=omni_header, header=None)\n",
    "\n",
    "        # same as above, think it just generates datetimes\n",
    "        data['DateTime'] = data.apply(\n",
    "            lambda row:\n",
    "            datetime.datetime(int(row.Year), 1, 1)\n",
    "            + datetime.timedelta(days=int(row.Day) - 1)\n",
    "            + datetime.timedelta(seconds=row.Hour*60*60+row.Minute*60),\n",
    "            axis=1)\n",
    "\n",
    "        # Select the data within our time range\n",
    "        data = data[(data.DateTime >= start_datetime)\n",
    "                    & (data.DateTime <= end_datetime)]\n",
    "\n",
    "        # Bodge broken data with NaN, easier to interpolate, pd is happier\n",
    "        data = data.replace(99.99, np.nan)\n",
    "        data = data.replace(999.9, np.nan)\n",
    "        data = data.replace(999.99, np.nan)\n",
    "        data = data.replace(9999.99, np.nan)\n",
    "        data = data.replace(99999.9, np.nan)\n",
    "        data = data.replace(9999999., np.nan)\n",
    "\n",
    "        # Make DateTime the index of the dataframe - ie the row labels\n",
    "        data.index = data['DateTime']\n",
    "\n",
    "        # In case we only wanted specific columns\n",
    "        if cols != 'All':\n",
    "            data = data[cols]\n",
    "\n",
    "        # store as pickle - MUCH faster loading, saves us doing this again\n",
    "        data.to_pickle(pkl_path)\n",
    "\n",
    "    return data\n",
    "\n",
    "def leapcheck(year):\n",
    "    '''Calculate whether the year is a leap year, return a True/False.\n",
    "    TBH I based this off psuedo-code from Wikipedia.\n",
    "    Args:\n",
    "      year: Integer of the year to check\n",
    "    Returns:\n",
    "      A boolean True/False on whether the year is a leap year.\n",
    "    '''\n",
    "    leap_year = False\n",
    "    if(year % 4 != 0):  # has to be divisible by 4\n",
    "        leap_year = False\n",
    "    elif(year % 100 == 0):\n",
    "        if(year % 400 == 0):  # year XX00 isn't leap unless multiple of 400\n",
    "            leap_year = True\n",
    "        else:\n",
    "            leap_year = False\n",
    "    else:\n",
    "        leap_year = True\n",
    "\n",
    "    return leap_year\n",
    "\n",
    "def import_omni_year(year):\n",
    "    \"\"\"Uses import_omni_month but concatenates it into an entire year.\n",
    "    See the dosctring for import_omni_month above.\n",
    "    Args:\n",
    "      year: the year of data to get\n",
    "    Returns:\n",
    "      data: a pandas DataFrame object containing the year of data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use pickles, MUCH faster than the .asc files\n",
    "    pkl_dir = pathlib.Path('/content/drive/My Drive/MSSL_DATA/pkl')\n",
    "    pkl_fname = str(year) + '.pkl'\n",
    "    pkl_path = pkl_dir / pkl_fname\n",
    "\n",
    "    # Look for the pickle\n",
    "    try:\n",
    "        year_df = pd.read_pickle(pkl_path)\n",
    "        print(\"Pickle found:\", pkl_fname)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pickle not found. Looking for local .asc files to create it.\")\n",
    "\n",
    "        # load in each month with import_omni_month, store the df in array\n",
    "        df_array = []\n",
    "        for i in range(0, 12):  # as 0->11, we need to use i+1 for months\n",
    "            this_month_df = import_omni_month(year, (i+1))\n",
    "            df_array.append(this_month_df)\n",
    "\n",
    "        # concat all the month's into one df\n",
    "        year_df = pd.concat(df_array)\n",
    "\n",
    "        # store as pickle - MUCH faster loading, saves us doing this again\n",
    "        year_df.to_pickle(pkl_path)\n",
    "\n",
    "    return year_df\n",
    "\n",
    "def import_storm_week(year, month, day):\n",
    "    \"\"\"\n",
    "    Imports the week of data surrounding a storm.\n",
    "    Can deal with storms spanning month boundary as imports multiple months.\n",
    "    Args:\n",
    "        year: the year of the data to import\n",
    "        month: the month of the data to import\n",
    "    Returns:\n",
    "        data: a pandas DataFrame object.\n",
    "    Ross Dobson, September 2020\n",
    "    \"\"\"\n",
    "\n",
    "    df_array = []\n",
    "    for i in range(month-1, month+2):\n",
    "        if (i == 13):\n",
    "            this_month_df = import_omni_month(year+1, 1)\n",
    "        elif (i == 0):\n",
    "            this_month_df = import_omni_month(year-1, 12)\n",
    "        else:\n",
    "            this_month_df = import_omni_month(year, i)\n",
    "\n",
    "        df_array.append(this_month_df)\n",
    "\n",
    "    # concat the three months together into one df_2003\n",
    "    storm_df = pd.concat(df_array)\n",
    "\n",
    "    # storm datetime - START of storm\n",
    "    storm_dt = datetime.datetime(year, month, day)\n",
    "    # start datetime -3 days\n",
    "    start_dt = storm_dt - datetime.timedelta(days=3)\n",
    "    # end datetime +4 days\n",
    "    end_dt = storm_dt + datetime.timedelta(days=4)\n",
    "    # this way we get 3 days either side\n",
    "    storm_df = storm_df[(storm_df.DateTime >= start_dt)\n",
    "                        & (storm_df.DateTime <= end_dt)]\n",
    "\n",
    "    return storm_df\n",
    "\n",
    "def storm_interpolator(my_df):\n",
    "    \"\"\"Straight-line interpolates gaps (NaNs) less than 15 minutes.\n",
    "    After that, it just removes any remaining NaNs inplace.\"\"\"\n",
    "    # df_resampled = my_df.resample(\n",
    "    #     '1T', loffset=datetime.timedelta(seconds=30.)).mean()\n",
    "    df_resampled = my_df.interpolate(method='linear', limit=15)\n",
    "    df_resampled.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "def storm_chunker(y_true, y_pred, y_pers, resolution='1h'):\n",
    "    \"\"\"\n",
    "    Splits the storm into chunks of timesteps (1h and 6h implemented)\n",
    "    If there's any gaps due to removed NaNs, it just means that hour will\n",
    "    have slightly less datapoints in it. E.g. the borders of the chunk will\n",
    "    still be 60 minutes apart, if not 60 datapoints apart.\n",
    "    Args:\n",
    "      y_true: the AL of true (discretized) AL to compare against\n",
    "      y_pred: the predicted values of AL from the model\n",
    "      y_pers: the persistence/AL history values\n",
    "    Returns:\n",
    "      chunks: array of 3-col dfs, each col is true, pred, pers\n",
    "    Author:\n",
    "      Ross Dobson 2020-10-06\n",
    "    \"\"\"\n",
    "\n",
    "    first_dt = y_true.index[0]  # start_dt cannot precede this \n",
    "    limit_dt = y_true.index[-1]  # end_dt cannot exceed this\n",
    "\n",
    "    year = int(first_dt.strftime(\"%Y\"))\n",
    "    month = int(first_dt.strftime(\"%m\"))\n",
    "    day = int(first_dt.strftime(\"%d\"))\n",
    "    hour = int(first_dt.strftime(\"%H\"))\n",
    "\n",
    "    # set it to the first (hopefully) full hour\n",
    "    if (resolution == '1h'):\n",
    "        start_dt = datetime.datetime(year, month, day, hour, 0)\n",
    "        end_dt = start_dt + datetime.timedelta(minutes=59)\n",
    "        interval = 1\n",
    "\n",
    "    elif (resolution == '6h'):\n",
    "        start_dt = datetime.datetime(year, month, day, hour, 0)\n",
    "        end_dt = start_dt + datetime.timedelta(hours=5, minutes=59)\n",
    "        interval = 6\n",
    "\n",
    "    else:\n",
    "        raise ValueError('This resolution', resolution, 'is not implemented.')\n",
    "\n",
    "    chunks = []\n",
    "    while (end_dt <= limit_dt):\n",
    "\n",
    "        # get the chunk\n",
    "        true_chunk = y_true.loc[start_dt:end_dt]\n",
    "        pers_chunk = y_pers.loc[start_dt:end_dt]\n",
    "        pred_chunk = y_pred.loc[start_dt:end_dt]\n",
    "\n",
    "        # make one df for all three, append\n",
    "        chunks.append(pd.concat(\n",
    "            [true_chunk, pred_chunk, pers_chunk], axis=1, sort=False))\n",
    "\n",
    "        # by iterating hours=1, we keep the XX:00 -> XX:59 spacing intact\n",
    "        start_dt += datetime.timedelta(hours=interval)\n",
    "        end_dt += datetime.timedelta(hours=interval)\n",
    "\n",
    "    # now that every chunk for this storm is generated, return\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SisOytDi42MQ",
    "outputId": "41546777-0e28-474e-944c-5358402a1860"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # IMPORTING THE DATA\n",
    "\n",
    "    # The method import_omni_year checks for Pickles itself\n",
    "    # so we do not need to enclose this in a try statement\n",
    "    data_2003 = import_omni_year(2003)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # IMPORTING OCTOBER AND NOVEMBER SEPERATELY TO ZOOM IN\n",
    "\n",
    "    # df_oct_2003 = import_omni_month(2003, 10)\n",
    "    # df_nov_2003 = import_omni_month(2003, 11)\n",
    "    # df_oct_nov_2003 = pd.concat([df_oct_2003, df_nov_2003])\n",
    "    # df_oct_nov_2003.to_pickle(oct_nov_pkl_path)  # store for future use\n",
    "\n",
    "    print(\"All 2003 data has been loaded.\")\n",
    "    # print(\"\\n2003 data:\")\n",
    "    # print(data_2003)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # PLOTTING THE PARAMETERS\n",
    "\n",
    "    # plot_vals = ['B_X_GSM', 'B_Y_GSM', 'B_Z_GSM', 'n_p', 'P', 'V', 'AL']\n",
    "\n",
    "    # for val in plot_vals:\n",
    "    #    # year_title = str(val) + ' in ' + str(year)\n",
    "    #    # data_2003.plot(x='DateTime', y=val, title=year_title)\n",
    "\n",
    "    #    # oct_nov_title = str(val) + ' in October and November 2003'\n",
    "    #    # df_oct_nov_2003.plot(x='DateTime', y=val, title=oct_nov_title)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # SCALER/TRANSFORMING THE DATA\n",
    "\n",
    "    # The features we care about\n",
    "    model_vals = ['B_X_GSM', 'B_Y_GSM', 'B_Z_GSM', 'n_p', 'P', 'V']\n",
    "\n",
    "    # get just that data in a new dataframe\n",
    "    df_2003 = data_2003[model_vals].copy()\n",
    "\n",
    "    # get the AL values separately, again in a new df/series\n",
    "    raw_AL = data_2003['AL'].copy()\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # PERSISTENCE TIME HISTORY OF AL\n",
    "\n",
    "    # Roll right, 30 minutes.\n",
    "    # E.g. 12:00-12:30 -> 12:30, 12:01-12:31 -> 12:31\n",
    "    pers_AL = raw_AL.copy()\n",
    "    pers_AL = pers_AL.rolling(30).min()\n",
    "\n",
    "    # create a copy before shifting - this is for persistence model later\n",
    "    disc_AL = pers_AL.copy()\n",
    "\n",
    "    # now, for our y/target data, we do a roll left by shifting\n",
    "    # e.g. 12:00 <- 12:00-12:30, 12:01 <- 12:01-12:31\n",
    "    disc_AL = disc_AL.shift(-30)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # SOLUTION TO NAN ISSUES FROM TIMESHIFTING\n",
    "\n",
    "    # drop the nans in the last 30 elements of the discretized AL\n",
    "    disc_AL.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "    # now, drop the last 30 minutes of all the other data so shapes stay equal\n",
    "    df_2003.drop(df_2003.tail(30).index, inplace=True)\n",
    "    pers_AL.drop(pers_AL.tail(30).index, inplace=True)\n",
    "    raw_AL.drop(raw_AL.tail(30).index, inplace=True)\n",
    "\n",
    "    # for the persistence model, we do the same, but they'll drop from head\n",
    "    pers_AL.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "    # drop the beginning 29 minutes of all other data to match (why 29? idk)\n",
    "    df_2003.drop(df_2003.head(29).index, inplace=True)\n",
    "    disc_AL.drop(disc_AL.head(29).index, inplace=True)\n",
    "    raw_AL.drop(raw_AL.head(29).index, inplace=True)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # ADD PERSISTENCE AS A FEATURE TO THE X ARRAY\n",
    "    df_2003.insert(6, \"AL_hist\", pers_AL.copy())\n",
    "    model_vals.append(\"AL_hist\")\n",
    "\n",
    "    # take cols and index for remaking DF after scaling\n",
    "    df_index = df_2003.index\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # CORRELATION MATRIX BEFORE SCALER\n",
    "\n",
    "    # add disc AL\n",
    "    df_2003.insert(7, \"disc_AL\", disc_AL)\n",
    "\n",
    "    # This is obsolete, but commented here as reminder of current state of\n",
    "    # the dataframes\n",
    "    # corr_pars = ['B_X_GSM', 'B_Y_GSM', 'B_Z_GSM', 'n_p', 'P', 'V',\n",
    "                 # 'AL_hist', 'disc_AL']\n",
    "\n",
    "    print(\"\\nCorrelation matrix before standardization:\\n\")\n",
    "    print(df_2003.corr())\n",
    "\n",
    "    # print(\"\\nCorrelation matrix for October & November 2003\\n\")\n",
    "    # corr_oct_nov_2003 = df_oct_nov_2003[plot_vals].corr()\n",
    "    # print(corr_oct_nov_2003)\n",
    "\n",
    "    # remove disc AL\n",
    "    df_2003 = df_2003.drop([\"disc_AL\"], axis=1)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # SCALE THE MAIN FEATURES\n",
    "\n",
    "    # fit the scaler, then transform the features\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaler = X_scaler.fit(df_2003)\n",
    "    X_scaled = X_scaler.transform(df_2003)\n",
    "\n",
    "    # need to add it back into a DF\n",
    "    df_2003 = pd.DataFrame(X_scaled, columns=model_vals, index=df_index)\n",
    "    print(\"\\n2003 features have been scaled.\")\n",
    "\n",
    "    # scale AL. We need to scale the discrete AL, and the raw AL\n",
    "    y_scaler = StandardScaler()\n",
    "\n",
    "    # reshape because StandardScaler needs (n_samples, n_features)\n",
    "    disc_AL = disc_AL.to_numpy().reshape(-1, 1)\n",
    "    raw_AL = raw_AL.to_numpy().reshape(-1, 1)\n",
    "\n",
    "    # fit the scaler to the data\n",
    "    y_scaler = y_scaler.fit(disc_AL)\n",
    "\n",
    "    # get our newly scaled y values\n",
    "    y_scaled = y_scaler.transform(disc_AL)\n",
    "    raw_scaled = y_scaler.transform(raw_AL)\n",
    "\n",
    "    # need to add it back into a DF\n",
    "    disc_AL = pd.DataFrame(y_scaled, columns=['disc_AL'], index=df_index)\n",
    "    raw_AL = pd.DataFrame(raw_scaled, columns=['raw_AL'], index=df_index)\n",
    "\n",
    "    print(\"2003 AL has been scaled.\")\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # CORR AFTER STANDARDISATION\n",
    "\n",
    "    # Add AL back in to the df\n",
    "    df_2003.insert(7, \"disc_AL\", disc_AL)\n",
    "\n",
    "    print(\"\\nCorrelation matrix after standardization:\\n\")\n",
    "    print(df_2003.corr())\n",
    "\n",
    "    # drop AL again, don't want it as a feature anymore\n",
    "    df_2003 = df_2003.drop([\"disc_AL\"], axis=1)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # VIOLIN PLOTS\n",
    "    # to check the distributions\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = sns.violinplot(data = df_2003)\n",
    "    _ = ax.set_xticklabels(df_2003.keys(), rotation=90)\n",
    "    ax.set_title('Violin plot of df_2003')\n",
    "    ax.set_xlabel('Feature')\n",
    "    ax.set_ylabel('Normalized')\n",
    "    # ---------------------------------------------------------------\n",
    "    # # MUTUAL INFORMATION\n",
    "\n",
    "    print(\"\\nMUTUAL INFORMATION:\")\n",
    "\n",
    "    # 1 year data crashes. Lets use 1 week, centred on 24h storm\n",
    "    storm_dt = datetime.datetime(2003, 10, 27, 0, 0, 0)  # start of storm\n",
    "    start_dt = storm_dt - datetime.timedelta(days=3)  # start of week: -3d\n",
    "    end_dt = storm_dt + datetime.timedelta(days=4)  # end of week: +4d\n",
    "\n",
    "    # make copy so we cant break anything\n",
    "    mi_2003 = df_2003.copy()\n",
    "\n",
    "    # reinsert AL for consistent NaN drop\n",
    "    mi_2003.insert(7, 'disc_AL', disc_AL)\n",
    "\n",
    "    # narrow to week\n",
    "    mi_2003 = mi_2003.loc[start_dt:end_dt]\n",
    "\n",
    "    # drop NaNs, MI doesn't like them\n",
    "    mi_2003.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "    mi_AL = mi_2003['disc_AL']\n",
    "    mi_2003 = mi_2003.drop(['disc_AL'], axis=1)\n",
    "\n",
    "    print(\"\\nExample scenario: n_p and P should have high MI:\")\n",
    "    print(mutual_info_regression(\n",
    "        mi_2003['P'].to_numpy().reshape(-1, 1), mi_2003['n_p']))\n",
    "\n",
    "    print(\"Discrete AL vs\", model_vals, \":\")\n",
    "    print(mutual_info_regression(mi_2003, mi_AL))\n",
    "\n",
    "    for i, feature in enumerate(model_vals):\n",
    "        print(\"\\nMutual information for\", feature, \"vs the others:\")\n",
    "        feature_array = model_vals.copy()\n",
    "        feature_array = np.delete(feature_array, i)\n",
    "        big_df = mi_2003.copy()\n",
    "        big_df = big_df.drop([feature], axis=1)\n",
    "        big_df = big_df.to_numpy()\n",
    "        small_df = mi_2003[feature]\n",
    "        small_df = small_df.to_numpy()\n",
    "        print(feature_array)\n",
    "        print(mutual_info_regression(big_df, small_df))\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # REMOVING UNNEEDED PARAMETERS\n",
    "\n",
    "    # removing n_p - in the words of Mayur\n",
    "    # \"by far weakest correlation with AL and a strong correlation with P\"\n",
    "    df_2003 = df_2003.drop([\"n_p\"], axis=1)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # INTERPOLATING GAPS\n",
    "\n",
    "    # to make the indexes match, we're gonna combine the whole damn thing\n",
    "    # and chuck it all through interpolator: df_2003 + disc_AL + raw_AL\n",
    "    df_2003.insert(5, \"disc_AL\", disc_AL)\n",
    "    df_2003.insert(6, \"raw_AL\", raw_AL)\n",
    "\n",
    "    # run the interpolation\n",
    "    df_2003 = storm_interpolator(df_2003)\n",
    "\n",
    "    print(\"\\n2003 features interpolated.\")\n",
    "\n",
    "    # seperate these two back off, drop them\n",
    "    raw_AL = df_2003['raw_AL'].copy()\n",
    "    disc_AL = df_2003['disc_AL'].copy()\n",
    "    df_2003 = df_2003.drop([\"raw_AL\", \"disc_AL\"], axis=1)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # PLOT HISTOGRAMS:\n",
    "\n",
    "    # hist_dir = pathlib.Path('Figures')\n",
    "    # pathlib.Path(hist_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # for i, param in enumerate(plot_vals):\n",
    "    #     plt.figure()\n",
    "    #     plt.hist(df_2003[param], bins=35)\n",
    "    #     plt.title('Histogram of '+param+' after StandardScaler')\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # KERAS PREP STEP:\n",
    "    pkl_dir = pathlib.Path('/content/drive/My Drive/MSSL_DATA/pkl')\n",
    "    \n",
    "    pkl_path = pkl_dir / ('2003_' + 'X' + '.pkl')\n",
    "    df_2003.to_pickle(pkl_path)\n",
    "\n",
    "    pkl_path = pkl_dir / ('2003_' + 'y' + '.pkl')\n",
    "    disc_AL.to_pickle(pkl_path)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # TRAIN TEST SPLIT\n",
    "\n",
    "    # Needs shape (n_samples, n_features)\n",
    "    # Split the data into two parts, one for training and testing\n",
    "    # 5 splits, which I think leads to 6 sets of data, 2 months each\n",
    "    # first 5 (k) are training, 6th (k+1)th one is testing\n",
    "\n",
    "    print(\"\\nFolding the data (cross-validation):\")\n",
    "    tscv = TimeSeriesSplit()\n",
    "    fold_counter = 0\n",
    "\n",
    "    for train_index, test_index in tscv.split(df_2003):\n",
    "\n",
    "        fold_counter += 1\n",
    "        print(\"Fold\", fold_counter, \"...\")\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)  # debug\n",
    "        X_train, X_test = df_2003.iloc[train_index], df_2003.iloc[test_index]\n",
    "        y_train, y_test = disc_AL.iloc[train_index], disc_AL.iloc[test_index]\n",
    "\n",
    "    print(\"Folding complete.\")\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # LINEAR REGRESSION\n",
    "\n",
    "    # create the linear regression object\n",
    "    regr = LinearRegression()\n",
    "\n",
    "    # train it on the training data\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # LINEAR REGRESSION PREDICTION AND SCORES\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    print(\"\\nLinear Regression R^2 score (best 1.0)\",\n",
    "          regr.score(X_test, y_test))\n",
    "    # print(r2_score(y_test, y_pred))\n",
    "\n",
    "    # put y pred in a dataframe, just makes life easier for future\n",
    "    y_test_index = y_test.index\n",
    "    y_pred = pd.DataFrame(y_pred, columns=['pred_AL'], index=y_test_index)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # GET PERSISTENCE AGAIN\n",
    "\n",
    "    pers_AL = df_2003['AL_hist'].copy()\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # EVALUATING OUR MODEL VS PERSISTENCE MODEL\n",
    "\n",
    "    def storm_metrics(y_true, y_pred, y_pers):\n",
    "        \"\"\"Runs various regression metrics from sklearn.metrics\n",
    "        Args:\n",
    "          y_true: The target values of discrete rolled-left AL\n",
    "          y_pred: The predicted values of discrete rolled-left AL from model\n",
    "          y_pers: The persistence (rolled-right) time history of AL\n",
    "        Returns:\n",
    "          None\n",
    "        \"\"\"\n",
    "\n",
    "        # Explained variance score (higher is better, best 1.0)\n",
    "        evs_true = explained_variance_score(y_true, y_pred)\n",
    "        evs_pers = explained_variance_score(y_true, y_pers)\n",
    "\n",
    "        # Mean absolute error (lower is better, best 0.0)\n",
    "        mean_true = mean_absolute_error(y_true, y_pred)\n",
    "        mean_pers = mean_absolute_error(y_true, y_pers)\n",
    "\n",
    "        # Mean squared error (lower is better, best 0.0)\n",
    "        mse_true = mean_squared_error(y_true, y_pred)\n",
    "        mse_pers = mean_squared_error(y_true, y_pers)\n",
    "\n",
    "        # not too affected by outliers - good choice of metric?\n",
    "        # Median absolute error (lower is better, best 0.0)\n",
    "        medi_true = median_absolute_error(y_true, y_pred)\n",
    "        medi_pers = median_absolute_error(y_true, y_pers)\n",
    "\n",
    "        # variance is dependent on dataset, might be a pitfall\n",
    "        # R2 coefficient of determination (higher=better), best 1.0\n",
    "        r2_true = r2_score(y_true, y_pred)\n",
    "        r2_pers = r2_score(y_true, y_pers)\n",
    "\n",
    "        return evs_true, evs_pers, mean_true, mean_pers, mse_true, mse_pers, medi_true, medi_pers, r2_true, r2_pers\n",
    "\n",
    "    metrics = [\"Explained variance score\",\n",
    "               \"Mean absolute error\",\n",
    "               \"Mean squared error\",\n",
    "               \"Median absolute error\",\n",
    "               \"R2 score\"]\n",
    "\n",
    "    metrics_desc = [\"higher is better, best 1.0\",\n",
    "                    \"lower is better, best 0.0\",\n",
    "                    \"lower is better, best 0.0\",\n",
    "                    \"lower is better, best 0.0\",\n",
    "                    \"higher is better, best 1.0\"]\n",
    "\n",
    "    # 2003 data - don't actually want to run metrics on this though\n",
    "    # storm_metrics(y_test, pred_df, pers_AL)\n",
    "\n",
    "    # ***************************************************************\n",
    "    # ***************************************************************\n",
    "    # IMPORTING THE VALIDATION STORMS, AND PREPARING THE DATA\n",
    "    # ***************************************************************\n",
    "    # ***************************************************************\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # IMPORT VALIDATION STORMS\n",
    "    # from doi:10.1002/swe.20056\n",
    "\n",
    "    print(\"\\nImporting the validation storms:\")\n",
    "\n",
    "    # DONT USE STORM 1 as its in the 2003 model training data!\n",
    "    # storm_1 = import_storm_week(2003, 10, 29)\n",
    "    storm_2 = import_storm_week(2006, 12, 14)\n",
    "    storm_3 = import_storm_week(2001, 8, 31)\n",
    "    storm_4 = import_storm_week(2005, 8, 31)\n",
    "    storm_5 = import_storm_week(2010, 4, 5)\n",
    "    storm_6 = import_storm_week(2011, 8, 5)\n",
    "\n",
    "    storm_array = [storm_2, storm_3, storm_4, storm_5, storm_6]\n",
    "\n",
    "    storm_str_array = [\"2006-12-14 12:00 to 2006-12-16 00:00\",\n",
    "                       \"2001-08-31 00:00 to 2001-09-01 00:00\",\n",
    "                       \"2005-08-31 10:00 to 2005-09-01 12:00\",\n",
    "                       \"2010-04-05 00:00 to 2010-04-06 00:00\",\n",
    "                       \"2011-08-05 09:00 to 2011-08-06 00:00\"]\n",
    "\n",
    "    # for pickling any storms later\n",
    "    storm_fname_array = [\"2006-12-14\",\n",
    "                         \"2001-08-31\",\n",
    "                         \"2005-08-31\",\n",
    "                         \"2010-04-05\",\n",
    "                         \"2011-08-05\"]\n",
    "\n",
    "    storm_start_array = [datetime.datetime(2006, 12, 14, 12, 0, 0),\n",
    "                         datetime.datetime(2001, 8, 31, 0, 0, 0),\n",
    "                         datetime.datetime(2005, 8, 31, 10, 0, 0),\n",
    "                         datetime.datetime(2010, 4, 5, 0, 0, 0),\n",
    "                         datetime.datetime(2011, 8, 5, 9, 0, 0)]\n",
    "\n",
    "    storm_end_array = [datetime.datetime(2006, 12, 16, 0, 0, 0),\n",
    "                       datetime.datetime(2001, 9, 1, 0, 0, 0),\n",
    "                       datetime.datetime(2005, 9, 1, 12, 0, 0),\n",
    "                       datetime.datetime(2010, 4, 6, 0, 0, 0),\n",
    "                       datetime.datetime(2011, 8, 6, 0, 0, 0)]\n",
    "\n",
    "    print(\"Validation storms imported successfully.\")\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # GET FEATURES DATA AND AL\n",
    "    # The features we care about - recall, no persistence yet\n",
    "    model_vals = ['B_X_GSM', 'B_Y_GSM', 'B_Z_GSM', 'n_p', 'P', 'V']\n",
    "\n",
    "    X_array = []\n",
    "    raw_array = []\n",
    "    for i, storm in enumerate(storm_array):\n",
    "        storm_index = storm.index  # save this for reconstructing DF later\n",
    "        X = storm[model_vals].copy()\n",
    "        raw = storm['AL'].copy()\n",
    "\n",
    "        # each storm's features to a df, append to X_array of dataframes\n",
    "        X_array.append(pd.DataFrame(X, columns=model_vals, index=storm_index))\n",
    "        raw_array.append(pd.DataFrame(raw, columns=['AL'], index=storm_index))\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # PERSISTENCE AND DISCRETIZATION (is that even a word)\n",
    "\n",
    "    disc_array = []\n",
    "    pers_array = []\n",
    "    df_index_array = []\n",
    "\n",
    "    # i.e. for each storm's raw AL dataframe\n",
    "    for i, raw in enumerate(raw_array):\n",
    "        pers_y = raw.copy()\n",
    "        pers_y = pers_y.rolling(30).min()  # roll right, 30 minute window\n",
    "        disc_y = pers_y.copy()\n",
    "        disc_y = disc_y.shift(-30)  # roll left for discrete AL\n",
    "\n",
    "        # drop the 30 minutes of NaNs from the shifting\n",
    "        disc_y.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "        # drop last 30 minutes of all others to match disc\n",
    "        pers_y.drop(pers_y.tail(30).index, inplace=True)\n",
    "        raw.drop(raw.tail(30).index, inplace=True)\n",
    "\n",
    "        # for persistence, the NaNs will dorp from head\n",
    "        pers_y.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "        # drop first 29 minutes to match\n",
    "        disc_y.drop(disc_y.head(29).index, inplace=True)\n",
    "        raw.drop(raw.head(29).index, inplace=True)\n",
    "\n",
    "        # add to the arrays outside the loop\n",
    "        pers_array.append(pers_y)\n",
    "        disc_array.append(disc_y)\n",
    "        raw_array[i] = raw.copy()\n",
    "\n",
    "        # store the indexes for later (same for disc, pers, raw)\n",
    "        df_index_array.append(disc_y.index)\n",
    "\n",
    "    # go through the five X feature arrays\n",
    "    for i, X in enumerate(X_array):\n",
    "\n",
    "        # make them match AL\n",
    "        X.drop(X.tail(30).index, inplace=True)\n",
    "        X.drop(X.head(29).index, inplace=True)\n",
    "\n",
    "        # ADD THEIR PERSISTENCE AS A FEATURE\n",
    "        X.insert(6, \"AL_hist\", pers_array[i].to_numpy())\n",
    "        X_array[i] = X.copy()\n",
    "\n",
    "    # outside the for loop so we dont add it more than once!\n",
    "    model_vals.append(\"AL_hist\")\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # STANDARD SCALING\n",
    "    # we use the same scalers we used earlier. X_scaler, y_scaler, raw_scaler\n",
    "    # recall X_array, raw_array. disc_array already defined, so we overwrite\n",
    "\n",
    "    for i, storm_index in enumerate(df_index_array):\n",
    "\n",
    "        # scale the main features\n",
    "        X_scaled = X_scaler.transform(X_array[i])\n",
    "\n",
    "        # each storm's features to a df, append to X_array of dataframes\n",
    "        X_array[i] = pd.DataFrame(X_scaled,\n",
    "                                  columns=model_vals, index=storm_index)\n",
    "\n",
    "        # scale disc AL\n",
    "        disc = disc_array[i].to_numpy()\n",
    "        disc = disc.reshape(-1, 1)\n",
    "        disc_scaled = y_scaler.transform(disc)\n",
    "\n",
    "        # scale raw AL\n",
    "        raw = raw_array[i].to_numpy()\n",
    "        raw = raw.reshape(-1, 1)\n",
    "        raw_scaled = y_scaler.transform(raw)\n",
    "\n",
    "        # store in the main arrays outside the loop\n",
    "        disc_array[i] = pd.DataFrame(disc_scaled, columns=['disc_AL'],\n",
    "                                     index=storm_index)\n",
    "        raw_array[i] = pd.DataFrame(raw_scaled, columns=['raw_AL'],\n",
    "                                    index=storm_index)\n",
    "\n",
    "    print(\"\\nValidation storm features and AL have been scaled.\")\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # REMOVING N_P\n",
    "    # \"by far weakest correlation with AL, and a strong correlation with P\"\n",
    "\n",
    "    for i, X in enumerate(X_array):\n",
    "        X = X.drop([\"n_p\"], axis=1)\n",
    "        X_array[i] = X\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # INTERPOLATE AND DROPNA IN THE TEST STORMS\n",
    "    # remember, no longer need to interpolate y because its discretized\n",
    "\n",
    "    # each storm will drop seperate dt, so store index to remake df\n",
    "    interped_index_array = []\n",
    "\n",
    "    for i, X in enumerate(X_array):\n",
    "\n",
    "        # to make the indexes match, we're gonna combine everything\n",
    "        # and chuck it all through the interpolator: df_200 + disc_AL + raw_AL\n",
    "        X.insert(5, \"disc_AL\", disc_array[i])\n",
    "        X.insert(6, \"raw_AL\", raw_array[i])\n",
    "\n",
    "        # interpolate the storm\n",
    "        X_array[i] = storm_interpolator(X)\n",
    "\n",
    "        # seperate these two back off, drop them\n",
    "        raw_array[i] = X_array[i]['raw_AL'].copy()\n",
    "        disc_array[i] = X_array[i]['disc_AL'].copy()\n",
    "        X_array[i] = X_array[i].drop(['raw_AL', 'disc_AL'], axis=1)\n",
    "\n",
    "        # store the new index for this storm, for later\n",
    "        interped_index_array.append(X_array[i].index)\n",
    "    print(\"\\nTest storms interpolated.\")\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # KERAS PREPATORY STEP:\n",
    "    # copy out storms 0 and 2 to use as testing data. We don't want all 5!\n",
    "    keras_X = [X_array[0], X_array[2]]\n",
    "    keras_raw = [raw_array[0], raw_array[2]]\n",
    "    keras_disc = [disc_array[0], raw_array[2]]\n",
    "\n",
    "    pkl_dir = pathlib.Path('/content/drive/My Drive/MSSL_DATA/pkl')\n",
    "    \n",
    "    pkl_path = pkl_dir / (storm_fname_array[0] + 'X' + '.pkl')\n",
    "    keras_X[0].to_pickle(pkl_path)\n",
    "    pkl_path = pkl_dir / (storm_fname_array[2] + 'X' + '.pkl')\n",
    "    keras_X[1].to_pickle(pkl_path)\n",
    "\n",
    "    pkl_path = pkl_dir / (storm_fname_array[0] + 'y' + '.pkl')\n",
    "    keras_disc[0].to_pickle(pkl_path)\n",
    "    pkl_path = pkl_dir / (storm_fname_array[2] + 'y' + '.pkl')\n",
    "    keras_disc[1].to_pickle(pkl_path)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # PREDICTING THE DATA\n",
    "    # remember we fitted the LinearRegression object regr on 2003 data\n",
    "\n",
    "    y_pred_array = []\n",
    "    for i, X in enumerate(X_array):\n",
    "        prediction = regr.predict(X)\n",
    "        pred_y = pd.DataFrame(\n",
    "            prediction, columns=[\"pred_AL\"], index=interped_index_array[i])\n",
    "        y_pred_array.append(pred_y)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # CHUNKING STORMS\n",
    "    chunk_res = '6h'\n",
    "\n",
    "    # will store the array of chunks for each of the storms\n",
    "    chunks_array = []\n",
    "\n",
    "    print(\"\\nChunking storms:\")\n",
    "\n",
    "    for i in range(0, len(storm_array)):\n",
    "        print(\"Chunking the week around storm\", storm_str_array[i])\n",
    "\n",
    "        # call the storm_chunker function to get array of chunks\n",
    "        chunks_array.append(\n",
    "            storm_chunker(disc_array[i], y_pred_array[i],\n",
    "                          X_array[i]['AL_hist'], resolution=chunk_res))\n",
    "\n",
    "    print(\"Validation storms have been chunked.\")\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # CALCULATING THE METRICS FOR EACH CHUNK\n",
    "\n",
    "    # iterate through each storm\n",
    "    for i, storm in enumerate(chunks_array):\n",
    "\n",
    "        metrics_array = [[], [], [], [], [], [], [], [], [], []]\n",
    "\n",
    "        # index array to plot the metric against\n",
    "        index_array = []\n",
    "\n",
    "        # for each chunk\n",
    "        for chunk in storm:\n",
    "\n",
    "            # run the metrics\n",
    "            if (len(chunk) != 0):\n",
    "                temp_metrics = storm_metrics(\n",
    "                    chunk['disc_AL'], chunk['pred_AL'], chunk['AL_hist'])\n",
    "\n",
    "                # append it into our metrics array\n",
    "                for j in range(0, len(temp_metrics)):\n",
    "                    metrics_array[j].append(temp_metrics[j])\n",
    "\n",
    "                # get just index[0] as we're appending to a whole array\n",
    "                index_array.append(chunk['disc_AL'].index[0].to_numpy())\n",
    "\n",
    "            else:\n",
    "                raise Exception(\"It's all gone a bit J.G. Ballard out there.\")\n",
    "\n",
    "        # OKAY lets get plotting for each metric!\n",
    "        for j in range(0, 5):\n",
    "\n",
    "            # this deals with model->pers->model->pers layout of metrics array\n",
    "            a = 2*j\n",
    "            b = a+1\n",
    "\n",
    "            fig, axs = plt.subplots(2, sharex=False)\n",
    "            fig.suptitle(metrics[j] + \" - \" + chunk_res + \" - \"\n",
    "                         + storm_str_array[i])\n",
    "\n",
    "            axs[0].plot(index_array, metrics_array[a], label='Model',\n",
    "                        marker='.')\n",
    "            axs[0].plot(index_array, metrics_array[b], label='Persistence',\n",
    "                        marker='.')\n",
    "            axs[0].axvline(storm_start_array[i], c='k', ls='--',\n",
    "                           label='Storm period')\n",
    "            axs[0].axvline(storm_end_array[i], c='k', ls='--')\n",
    "            axs[0].axhline(0, c='k', alpha=0.5, ls='dotted')\n",
    "            axs[0].set_ylabel('Metric score: ' + metrics_desc[j])\n",
    "            axs[0].set_xlabel('DateTime')\n",
    "\n",
    "            # get the ylim so we can check if the default is okay\n",
    "            old_ylim = axs[0].get_ylim()\n",
    "            new_low = old_ylim[0]\n",
    "            new_top = old_ylim[1]\n",
    "\n",
    "            if (new_low <= -10):\n",
    "                new_low = -10\n",
    "\n",
    "            if (new_top >= 10):\n",
    "                new_top = 10\n",
    "\n",
    "            # if a best 1.0 metric, no need to go above 2\n",
    "            if j == 0 or j == 4:\n",
    "                new_top = 2\n",
    "                new_low = -8\n",
    "                axs[0].set_yticks(np.arange(-8, 3, step=1))\n",
    "\n",
    "            # if a best 0.0 metric, no need to go below -0.5\n",
    "            if j == 1 or j == 2 or j == 3:\n",
    "                new_low = -0.5\n",
    "\n",
    "            # set the ylim again\n",
    "            axs[0].set_ylim((new_low, new_top))\n",
    "\n",
    "            # generate the legend, auto-location\n",
    "            axs[0].legend(loc='best')\n",
    "\n",
    "            # the subplot - disc AL vs raw AL vs persistence AL\n",
    "            start = index_array[0]\n",
    "            end = index_array[-1]\n",
    "\n",
    "            axs[1].plot(disc_array[i].loc[start:end], label='disc', alpha=0.5)\n",
    "            axs[1].plot(raw_array[i].loc[start:end], label='raw', alpha=0.5)\n",
    "\n",
    "            axs[1].axvline(storm_start_array[i], c='k', ls='--',\n",
    "                           label='Storm period')\n",
    "            axs[1].axvline(storm_end_array[i], c='k', ls='--')\n",
    "\n",
    "            axs[1].legend(loc='best')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "n0JINWD6dFiS",
    "outputId": "95414a88-0281-4ebd-edbc-50d8fa92fd54"
   },
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    \"\"\"Thanks to Andy Smith for modifications to exclude labels from inputs\"\"\"\n",
    "    def __init__(self, \n",
    "                 input_width, label_width, shift, data, label_columns=None):\n",
    "        \n",
    "        # Store the raw data.\n",
    "        self.data_df = data\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in enumerate(data.columns)}\n",
    "        ####Marking feature column so it isnt included\n",
    "        # self.feature_indices = {name: i for i, name in enumerate(train_df.columns) if name not in self.label_columns}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    \n",
    "    def split_window(self, features):\n",
    "        ### Original\n",
    "        # inputs = features[:, self.input_slice, :]\n",
    "        ### Adjusted to remove target (can work for >1 target too)\n",
    "        inputs = features[:, self.input_slice, :-1]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "        \n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=False,\n",
    "            batch_size=1440,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "        \n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARib9AUyekiX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdoiCSCNEz-w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfIbVRLpBkaY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MSSL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
